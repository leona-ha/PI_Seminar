{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einführung in Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dies ist eine kleine Einführung in Natural Language Processing anhand der Freitext-Features unseres OSMI2016 Datensatzes. Wir werden uns zunächst Supervised Learning und anschließend Unsupervised Learning anschauen. \n",
    "\n",
    "**Natural Language Processing** ist eine Abzweigung von Machine Learning, die sich mit der Verarbeitung, Analyse und manchmal auch der Erzeugung menschlicher Sprache (\"Natural Language\") befasst. Das bietet sich vor allem bei großen Datensätzen an, bei denen die menschliche Bearbeitung ineffizient wäre.  Aktuelle Beispiele von Natural Language Processing sind Chatbots (z.B. Akexa, Siri). Zudem wird Sentiment Analysis gerne genutzt, um Reviews zu Produkten zu verarbeiten, oder die allgemeine Stimmung zu einem Thema herauszufinden (z.B. auf Twitter: https://monkeylearn.com/blog/sentiment-analysis-of-twitter/).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wordcloud\n",
    "import pyLDAvis.sklearn\n",
    "from pylab import bone, pcolor, colorbar, plot, show, rcParams, savefig\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Display plots inside the notebook\n",
    "%matplotlib inline\n",
    "sns.set(style=\"white\", palette=\"pastel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Vorbereitung des Datensatzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz einlesen und Column Names kleinschreiben\n",
    "\n",
    "df_2016 = pd.read_csv(\"OSMI_2016_kurz.csv\")\n",
    "df_2016.columns = map(str.lower, df_2016.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename_dict aus vorherigen Analysen verwenden\n",
    "\n",
    "rename_dict = {'are you self-employed?': \"Employment\", \n",
    "              'does your employer provide mental health benefits as part of healthcare coverage?':\"Ment_Benefit\",\n",
    "              'has your employer ever formally discussed mental health (for example, as part of a wellness campaign or other official communication)?': \"Ment_Discuss\",\n",
    "               'do you think that discussing a mental health disorder with your employer would have negative consequences?': \"Ment_Consequence\",\n",
    "               'do you think that discussing a physical health issue with your employer would have negative consequences?': \"Phys_Consequence\",\n",
    "               'would you feel comfortable discussing a mental health disorder with your coworkers?': \"Discuss_Coworkers\",\n",
    "               'would you feel comfortable discussing a mental health disorder with your direct supervisor(s)?': \"Discuss_Supervisor\",\n",
    "               'do you feel that your employer takes mental health as seriously as physical health?': \"Ment_vs_Phys\",\n",
    "               'have you heard of or observed negative consequences for co-workers who have been open about mental health issues in your workplace?': \"Obs_Consequence\",\n",
    "               'if you have been diagnosed or treated for a mental health disorder, do you ever reveal this to coworkers or employees?': \"Reveal_Treatment\",\n",
    "               'do you believe your productivity is ever affected by a mental health issue?': \"Productivity\",\n",
    "               'would you be willing to bring up a physical health issue with a potential employer in an interview?': \"Interview_phys1\",\n",
    "               'why or why not?': \"Interview_phys2\",\n",
    "               'would you bring up a mental health issue with a potential employer in an interview?': \"Interview_psych1\",\n",
    "               'why or why not?.1': \"Interview_psych2\",\n",
    "               'do you feel that being identified as a person with a mental health issue would hurt your career?': \"Career_Consequence\",\n",
    "               'do you think that team members/co-workers would view you more negatively if they knew you suffered from a mental health issue?': \"Coworkers_view\",\n",
    "               'how willing would you be to share with friends and family that you have a mental illness?': \"Share_friends\",\n",
    "               'have you observed or experienced an unsupportive or badly handled response to a mental health issue in your current or previous workplace?': \"Obs_Response1\",\n",
    "               'have your observations of how another individual who discussed a mental health disorder made you less likely to reveal a mental health issue yourself in your current workplace?': \"Obs_Response2\",\n",
    "               'have you ever sought treatment for a mental health issue from a mental health professional?': \"Treatment\",\n",
    "               'if you have a mental health issue, do you feel that it interferes with your work when being treated effectively?': \"Interferes1\",\n",
    "               'if you have a mental health issue, do you feel that it interferes with your work when not being treated effectively?': \"Interferes2\",\n",
    "               'what is your age?': \"Age\", \n",
    "               'what is your gender?': \"Gender\",\n",
    "               'what country do you live in?': \"Country\",\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns mithilfe des Rename_dicts umbenennen\n",
    "\n",
    "df = df_2016.rename(columns = rename_dict, errors = \"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wir nehmen nur die Freitext-Spalten und die Ja/Nein Fragen (\"Would you discuss a mental/physical issue...\")\n",
    "# in unseren NLP Datensatz auf \n",
    "\n",
    "df_NLP = df[[\"Interview_phys1\", \"Interview_phys2\", \"Interview_psych1\", \"Interview_psych2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wie viele fehlende Werte gibt es?\n",
    "\n",
    "df_NLP.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalerweise nicht empfehlenswert, aber hier können wir die fehlenden Werte nicht ersetzen. \n",
    "# wir droppen also alle Zeilen, in denen es entweder bei Interview_phys2 oder Interview_psych2 fehlende Werte gibt.\n",
    "\n",
    "df_NLP = df_NLP.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ändere die gesamten Freitexteinträge auf Kleinschrift\n",
    "\n",
    "df_NLP = df_NLP.applymap(lambda s:s.lower() if type(s) == str else s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vorbereitung von Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit wir Machine Learning auf Text-Corpora anwenden können müssen wir sie in eine **numerische Form** bringen. Dazu sind ein paar Vorbereitungsschritte notwendig. Zunächst müssen wir ein Sprachpaket herunterladen, damit wir die Wörter im Text in Wort-Tokens umwanden können (mehr dazu gleich). Die **Spacy**-Library bietet diese Sprachpakete auf verschiedenen Sprachen und Größen an. In unserem Fall laden wir das mittelgroße Englisch-Paket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das englische Spacy Sprach Paket müssen Sie vorher runterladen \n",
    "# https://spacy.io/usage/models\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wichtige NLP-Konzepte 1\n",
    "\n",
    "- **Tokenization**: Ein fundamentaler Preprocessing-Step für einen Großteil der NLP-Anwendungen. Zerlegt Text-Daten in kleinere Elemente (z.B. Wörter, Sätze,...) die als diskrete Elemente gezählt werden können. In unserem Fall zerlgen wir den Text in Wörter. \n",
    "- **Lemmatisation**: Wörter werden auf ihren Grundform reduziert, z.B. \"Houses\" --> \"House\", \"Thought\" --> \"Think\" etc. \n",
    "- **Stop-Words**: Wörter, die in einer Sprache so häufig vorkommen, dass sie nicht viel Bedeutung zum Thema eines Texts beitragen. Beispiele sind \"a\", \"the\", \"do\" etc. In unserem Fall werden wir diese Stop-Words entfernen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Text Corpora \n",
    "df_NLP[\"Interview_phys_NLP\"] = [nlp(s) for s in df_NLP[\"Interview_phys2\"]]\n",
    "df_NLP[\"Interview_psych_NLP\"] = [nlp(s) for s in df_NLP[\"Interview_psych2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NLP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explorative Datenanalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show universal pos (=part of speech) tags\n",
    "for corpus in df_NLP.Interview_psych_NLP[:1]:\n",
    "    print(' '.join('{word}/{tag}'.format(word=t.orth_, tag=t.pos_) for t in corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show dependency arcs\n",
    "for corpus in df_NLP.Interview_psych_NLP[2:3]:\n",
    "    print('\\n'.join('{child:<8} <{label:-^7} {head}'.format(child=t.orth_, \\\n",
    "                                                            label=t.dep_, head=t.head.orth_) for t in corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show named entities \n",
    "\n",
    "spacy.displacy.render(df_NLP.Interview_psych_NLP[1], style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization & Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die nachfolgende Funktion wendet Tokenization und Lemmatization auf unsere Text Corpora an und entfern Stop-Words und Satzzeichen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacify_my_text(text, custom_stop=None, lemmatize=True):\n",
    "    ''' Loops through list of text-corpora, tokenizes and lemmatizes words and\n",
    "    removes stop words and punctutations.\n",
    "    Returns lemmatized list of words'''\n",
    "\n",
    "    spacyfied = []\n",
    "    original = list(nlp.Defaults.stop_words) # list of stop words\n",
    "    original.append(' ') # adds space to stop word list\n",
    "    for p in string.punctuation: # adds punctuations to stop-list to remove them\n",
    "        original.append(p)\n",
    "\n",
    "    if custom_stop: # adds custom stop words to basic list\n",
    "        for i in custom_stop:\n",
    "            if i not in original:\n",
    "                original.append(i)\n",
    "\n",
    "    parsed_sentence = nlp(text.lower())\n",
    "    treated_sentence = ''\n",
    "\n",
    "# With SpaCy we can access each word’s base form with a token’s .lemma_ method\n",
    "\n",
    "    for token in parsed_sentence:\n",
    "        if str(token) not in original:\n",
    "            if lemmatize:\n",
    "                treated_sentence += str(token.lemma_) + ' '\n",
    "            else:\n",
    "                treated_sentence += str(token) + ' '\n",
    "    spacyfied.append(treated_sentence.strip())\n",
    "\n",
    "    return spacyfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wende die spacify_my_text Funktion auf unsere Freitext-Variablen an\n",
    "# Ich ergänze hier \"100\" zur Stop-List\n",
    "\n",
    "custom_stop= [\"100\"]\n",
    "\n",
    "df_NLP[\"Psych_spacified\"] = df_NLP[\"Interview_psych2\"].apply(lambda s:spacify_my_text(s, custom_stop=custom_stop))\n",
    "df_NLP[\"Phys_spacified\"] = df_NLP[\"Interview_phys2\"].apply(lambda s:spacify_my_text(s,custom_stop=custom_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NLP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordcloud für unsere \"Psychological Issue\" Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_psych = df_NLP[\"Psych_spacified\"].tolist()\n",
    "wordcloud_psych = [item for sublist in wordcloud_psych for item in sublist]\n",
    "wordcloud_psych = ' '.join(wordcloud_psych)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros((500, 500, 3), np.uint8)\n",
    "mask[50:70, 50:70,:] = 255  # masked out area\n",
    "cloud = wordcloud.WordCloud(background_color=\"white\",\n",
    "                max_words=50,\n",
    "                mask=mask,\n",
    "                collocations= False,  # calculates frequencies\n",
    "                contour_color='steelblue').generate(wordcloud_psych)\n",
    "                # stop words are removed!\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.savefig('cloud.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordcloud für unsere \"Physiological Issue\" Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_phys = df_NLP[\"Phys_spacified\"].tolist()\n",
    "wordcloud_phys = [item for sublist in wordcloud_phys for item in sublist]\n",
    "wordcloud_phys = ' '.join(wordcloud_phys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros((500, 500, 3), np.uint8)\n",
    "mask[50:70, 50:70,:] = 255  # masked out area\n",
    "cloud = wordcloud.WordCloud(background_color=\"white\",\n",
    "                max_words=50,\n",
    "                mask=mask,\n",
    "                collocations= False,  # calculates frequencies\n",
    "                contour_color='steelblue').generate(wordcloud_phys)\n",
    "                # stop words are removed!\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.savefig('cloud.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wichtige NLP-Konzepte 2\n",
    "\n",
    "Nach der Vorverarbeitung müssen wir den Text in einen aussagekräftigen Vektor/Array von Zahlen umwandeln. Die **Bag-of-Words** Methode ist eine Repräsentation, die das Vorkommen von Worten in einem Dokument beschreibt. Dabei stellt das Vokabular von Wörtern die Features/Spalten dar, während jede Zeile ein Text-Corpus ist. \n",
    "\n",
    "Die Einträge in der Matrix sind dann ein Maß für das Vorkommen der Wörter, meist entweder die absolute Anzahl oder lediglich das binäre Vorkommen (Ja/Nein). Warum wird es \"Bag of Words\" genannt? Weil jegliche Information über die Struktur oder Reihenfolge der Wörter in den Texten verworfen wird und sich das Modell nur darauf bezieht ob/ wie oft ein Wort in den Texten vorkommt. \n",
    "\n",
    "Ein Problem beim \"Bag of Words\"-Ansatz besteht darin, dass hochfrequente Wörter anfangen, im Dokument zu dominieren, aber möglicherweise nicht so viel \"Informationsgehalt\" haben. Außerdem wird dadurch längeren Texten mehr Gewicht verliehen als kürzeren Texten.\n",
    "\n",
    "Ein Ansatz besteht darin, die Häufigkeit von Wörtern danach zu skalieren, wie oft sie in allen Dokumenten vorkommen, so dass die Punktzahlen für häufige Wörter wie \"Person\", die ebenfalls häufig in allen Dokumenten vorkommen, geringer werden. Dieser Ansatz zur Bewertung wird Term-Frequency-Inverse Document Frequency, oder kurz **TF-IDF**, genannt. Dabei gilt:\n",
    "- Term Frequency: is a scoring of the frequency of the word in the current document\n",
    "- TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "- Inverse Document Frequency: is a scoring of how rare the word is across documents.\n",
    "- IDF = 1+log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_psych = df_NLP.Psych_spacified.tolist()\n",
    "X_psych = [item for sublist in X_psych for item in sublist]\n",
    "X_phys = df_NLP.Phys_spacified.tolist()\n",
    "X_phys = [item for sublist in X_phys for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer/ Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_psych = CountVectorizer(min_df=1)\n",
    "X_psych_cv= cv_psych.fit_transform(X_psych)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=1)\n",
    "X_phys_cv= cv.fit_transform(X_phys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_ps= TfidfVectorizer(min_df=1)\n",
    "X_psych_tf= tf_ps.fit_transform(X_psych)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_ph= TfidfVectorizer(min_df=1)\n",
    "X_phys_tf= tf_ph.fit_transform(X_phys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_vector_psych=X_psych_tf[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_vector_phys=X_phys_tf[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show calculated weights of word importance: Psychological Issues\n",
    "tdf = pd.DataFrame(first_vector_psych.T.todense(), index=tf_ps.get_feature_names(), columns=[\"tfidf\"])\n",
    "tdf.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show calculated weights of word importance: Physiological Issues\n",
    "tdf = pd.DataFrame(first_vector_phys.T.todense(), index=tf_ph.get_feature_names(), columns=[\"tfidf\"])\n",
    "tdf.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Supervised Text Classification vs. Unsupervised Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised Text Classification** ist ein Problem, bei dem wir eine feste Anzahl von Klassen/Kategorien haben und jeder Text einer diese Kategorien zugeordnet wird. \n",
    "\n",
    "Im Gegensatz dazu gibt es beim **Unsupervised Text Clustering** keine vorgegebenen Klassen/Kategorien. Das Clustering verfolgt dabei das Ziel, die Texte so zu gruppieren, dass Texte im selben Cluster einander ähnlicher sind als Texte in anderen Clustern. Das Modell deckt verborgene Strukturen innerhalb und zwischen den Dokumenten auf - es ist die Aufgabe der Analytiker diese Informationen ensprechend zu interpretieren und die Kategorien zu benennen. \n",
    "\n",
    "Wir beginnen zunächst mit Supervised Classification, indem wir ein Modell trainieren anhand der Freitext-Inputs vorherzusagen, ob man seinen Vorgesetzten von einer psychischen Erkrankung erzählen würde (ja/nein/vielleicht).\n",
    "\n",
    "**y**: 'would you bring up a mental health issue with a potential employer in an interview?' (Yes/Maybe/No)\n",
    "\n",
    "**X**: 'Why or why not' (Freitext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Text classification (Supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leider extrem große Class Imbalance --> könnte problematisch werden\n",
    "df_NLP.groupby(\"Interview_psych1\")[\"Interview_psych1\"].count().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NLP[\"X_psych\"] = X_psych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lege X und y fest und erstelle einen Train-Test-Split\n",
    "\n",
    "X= df_NLP['X_psych']\n",
    "y= df_NLP['Interview_psych1']\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest= train_test_split(X,y,random_state=35, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Vectorization and TF-IDF transformation to Xtrain and Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df= 1)\n",
    "Xtrain_cv= cv.fit_transform(Xtrain)\n",
    "\n",
    "tf= TfidfTransformer()\n",
    "Xtrain_tf= tf.fit_transform(Xtrain_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier müssen wir nur transformen, da der Count Vectorizer und der TFIDF-Vectorizer an unserem Train Datensatz gefittet wurde\n",
    "Xtest_cv= cv.transform(Xtest)\n",
    "Xtest_tf= tf.transform(Xtest_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a Multinomial Naive Bayes model on our transformed vector (X) and Response labels (y) and predict Response for Xtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes bezeichnet eine Reihe von Algorithmen, mit denen Kategorien vorhergesagt werden können. Diese basieren auf die Anwendung des Bayes-Theorems, mit der (naiven) Annahme, dass jedes Merkmal unabhängig von den anderen ist. Naive Bayes sind probabilistische Classifier, d.h. die Wahrscheinlichkeit, dass ein Input in eine bestimmte Kategorie fällt wird für jede Kategorie berechnet. Der Output des Modells ist dann die Kategorie, mit der höchsten Wahrscheinlichkeit. \n",
    "\n",
    "Naive Bayes Classifier sind relativ simpel, aber dennoch schnell und verhältnismäßig präzise - ein optimales Einstiegsmodell!\n",
    "\n",
    "Möchten Sie selber einen Naive Bayes Classifier bauen? Dieses Tutorial zeigt ihnen, wie es geht:\n",
    "https://chrisalbon.com/machine_learning/naive_bayes/naive_bayes_classifier_from_scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "m = MultinomialNB(alpha=1.0)\n",
    "m.fit(Xtrain_tf, ytrain)\n",
    "ypred= m.predict(Xtest_tf) #predictions\n",
    "proba= m.predict_proba(Xtest_tf) #probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das Ergebnis zeigt die Precision Scores für unsere 3 Klassen. Wir sehen, dass die Precision für die \"Yes\" Klasse\n",
    "# 0 ist - das liegt in diesem Fall daran, dass in unserem zufällig gewählten Prediction Datensatz keine \"Yes\" Kategorie enthalten ist,\n",
    "precision_score(ytest, ypred, average=None, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(ytest, ypred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(ypred, ytest, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.score(Xtest_tf, list(ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare= pd.DataFrame(zip(list(ytest), list(Xtest), list(proba[:,0]), list(proba[:,1]),list(proba[:,2])), index= list(ypred), \n",
    "                      columns= ['actual_categorie', 'text', 'prob_no', 'prob_maybe', 'prob_yes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die hohe **Class Imbalance** führt dazu, dass das Modell dazu geneigt ist, die stärker vertretene Klasse mit höherer Wahrscheinlichkeit vorherzusagen. Lösungen, die Sie ausprobieren können: unterrepräsentierte Kategorie upsamplen (z.B. Kategorie \"Yes\" und \"Maybe\" zusammenfügen), oder überrepräsentierte Kategorie downsamplen (Nur empfehlenswert, bei extrem großen Datensätzen. I.d.R. versucht man alle vorhandenen Datenpunkte zu nutzen). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Unsupervised Text Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim Unsupervised Text Clustering haben wir keine vorgegebenen Klassen/Kategorien. Wir versuchen also, mithilfe von Machine Learning Themen in den Daten zu finden. \n",
    "\n",
    "Der am weitesten verbreitete und einfachste Clustering-Algorithmus, den es gibt, ist das **K-Means-Clustering**. Bei diesem Algorithmus teilen Sie den Algorithmen mit, wie viele mögliche Cluster (oder K) es im Datenbestand gibt. Der Algorithmus verschiebt dann iterativ die k-Zentren und wählt die Datenpunkte aus, die diesem Centroid im Cluster am nächsten liegen. Die Eingaben in den Algorithmus sind die Anzahl der Cluster Κ und der Datensatz. Der Datensatz ist eine Sammlung von Merkmalen für jeden Datenpunkt. Der Algorithmus beginnt mit ersten Schätzungen für die K Centroids, die entweder zufällig generiert oder zufällig aus dem Datensatz ausgewählt werden können. Der Algorithmus iteriert dann zwischen zwei Schritten:\n",
    "\n",
    "1. **Data assignment Step**: Jeder Centroid definiert eines der Cluster. In diesem Schritt wird jeder Datenpunkt seinem nahesten Centroid zugewiesen (die \"Nähe\" basiert auf der quadrierten Euclidean Distance)\n",
    "\n",
    "2. **Centroid Update Step**: In diesem Schritt werden die Centroide neu berechnet, indem der Mittelwert aller Datenpunkte, die diesem Centroid zugeordnet sind, gebildet wird. \n",
    "\n",
    "Der Algorithmus iteriert zwischen den Schritten eins und zwei, bis ein Abbruchkriterium erfüllt ist (z.B. keine Datenpunkte ändern die Cluster oder eine bestimmte maximale Anzahl von Iterationen wird erreicht)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_of_squared_distances = []\n",
    "K = range(1,15)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(X_psych_tf)\n",
    "    Sum_of_squared_distances.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit steigendem k tendiert die Summe der quadrierten Distanz gegen Null. Stellen Sie sich vor, wir setzen k auf seinen Maximalwert n (wobei n die Anzahl der Stichproben ist), dann bildet jede Stichprobe ihren eigenen Cluster, was bedeutet, dass die Summe der quadrierten Abstände gleich Null ist. In diesem Fall haben wir keine optimale Elbow Curve. Wir sehen aber einen leichten Knick bei 3 Clustern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predefine 3 situation categories\n",
    "true_k = 3\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=300, n_init=10)\n",
    "model.fit(X_psych_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = tf_ps.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgenden drei Cluster wurden aus den Textdaten extrahiert. Es liegt nun an uns, diese Cluster zu Themen/ Kategorien zusammenzusammen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" %i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(\" %s\" % terms[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA ist ein bag-of-words Algorithmus, der uns hilft, automatisch Themen in einem Set von Text-Dokumenten aufzuspüren. Dazu ein Beispiel:\n",
    "\n",
    "- I wish I had a Lexus to drive to the game next week\n",
    "- My trusty Honda needs an oil change\n",
    "- Becky needs the Honda to go pick up the desk\n",
    "- Jimmy fell off his chair laughing\n",
    "- John put the couch and desk up for sale on eBay\n",
    "\n",
    "Wenn Sie gezwungen wären, zwei Themen (Thema A und B) zu wählen, könnten Sie die Topic Representation über die Sätze hinweg folgendermaßen verteilen:\n",
    "\n",
    "- Sentence 1 & 2: 100% about topic A\n",
    "- Sentence 4 & 5: 100% about topic B\n",
    "- Sentence 3: 50% topic A, 50% topic B\n",
    "\n",
    "Die Wortverteilung über die Themen für alle Sätze gliedert sich weiter auf:\n",
    "\n",
    "- Topic A: 25% Honda, 15% Lexus, 15% drive, 15% oil change \n",
    "- Topic B: 30% Desk, 20% chair, 20% couch \n",
    "\n",
    "Wir können interpretieren, dass es sich bei Thema A um Autos und bei Thema B um Möbel handelt. Mit der LDA können wir diese Themen automatisch finden. \n",
    "Textdokumente sind immer eine Mischung aus verschiedenen Themen, und innerhalb dieser Themen treten bestimmte Wörter mit einer bestimmten Wahrscheinlichkeit auf. Das Herzstück der LDA ist eine **Dirichlet-Verteilung** - eine Wahrscheinlichkeitsverteilung, ähnlich wie z.B. die Normalverteilung.\n",
    "\n",
    "Jedes Dokument stellt eine Wahrscheinlichkeitsverteilung von Themen und jedes Thema stellt eine Wahrscheinlichkeitsverteilung von bestimmten Wörtern dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wählen Sie die Anzahl von Themen aus, die Sie in Ihren Textdokumenten vermuten\n",
    "NUM_TOPICS = 3\n",
    "\n",
    "# Latent Dirichlet Allocation Model\n",
    "lda = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, learning_method='online',verbose=True)\n",
    "X_lda = lda.fit_transform(X_psych_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for printing keywords for each topic\n",
    "def selected_topics(model, vectorizer, topic_n):\n",
    "    print(f\"Extracted Topic for model:{model}\")\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic:{idx}\")\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-topic_n - 1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_topics(lda, cv_psych, NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Topics\n",
    "pyLDAvis.enable_notebook()\n",
    "dash = pyLDAvis.sklearn.prepare(lda, X_psych_cv, cv_psych, mds='tsne')\n",
    "dash"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
