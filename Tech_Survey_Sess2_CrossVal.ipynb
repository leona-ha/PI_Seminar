{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning & Deep Learning: Cross-Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein kleines Add-On, in dem ich Ihnen zeige, wie man die K-fold Cross-Validation anwendet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 1: Importiere relevante Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn ist das bekanntest Python Package für Machine Learning. Es enthält die wichtigsten Modelle und Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import helpers\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Validation libraries\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, recall_score, mean_squared_error, precision_score, precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "\n",
    "# Display plots inside the notebook\n",
    "%matplotlib inline\n",
    "sns.set(style=\"white\", palette=\"pastel\")\n",
    "\n",
    "# Ignore warning related to pandas_profiling\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Display all dataframe columns in outputs (it has 63 columns, which is wider than the notebook)\n",
    "# This sets it up to display with a horizontal scroll instead of hiding the middle columns\n",
    "pd.set_option('display.max_columns', 100) \n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 2: Train und Test Datensatz festlegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mit der Pickle Funktion packen wir den fertig bearbeiteten Dataframe in eine Datei, die wir nächste Woche einfach \n",
    "# importieren können. \n",
    "X = pd.read_pickle(\"X_df.pkl\")\n",
    "y = pd.read_pickle(\"y_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5 # wenn wir den seed festlegen können wir den random-split später reproduzieren\n",
    "test_size = 0.33 # d.h. 33% des Datensatzes werden zufällig ausgewählt und dem Test- Datensatz zugeordnet.\n",
    "\n",
    "# Durch den Train Test Split teilen wir den Datensatz in einen Train-Datensatz und einen Test-Datensatz auf. \n",
    "# Das Modell wird auf dem Train-Datensatz trainiert und am Test-Datensatz getestet.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vorbereitung zur Evaluierung von Modellen:\n",
    "1. An den gleichen Daten trainieren und testen: Wir wissen nicht, ob das Modell auch für andere Daten gute Vorhersagen trifft. \n",
    "2. **Train/Test Split**: Teilt den Datensatz in zwei Teile auf, sodass an unterschiedlichen Daten gelernt und getestet wird. Bessere Schätzung der Out-of-Sample-Performance (=Generalisierung) als 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 K-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei der K-fold Cross-Validation wir der Datensatz in K verschiedene Subsamples (=folds) unterteilt. Jeder fold wird dabei einmal als Test-Datensatz benutzt (während die restlichen **K-1** folds zum Training genutzt werden). So kann die Modell-Güte an verschiedenen Datensätzen getestet werden und ist somit objektiver. Das Ergebnis sind **K** Accuracy-Scores, aus denen i.d.R. der Mittelwert gebildet wird. \n",
    "\n",
    "\n",
    "<img src=\"img/cross_val.gif\" alt=\"learning\" style=\"width: 500px;\"/>\n",
    "Image Source: https://genome.tugraz.at/proclassify/help/pages/XV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 3: Modelle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Logistische Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell anlegen. Der Hyperparameter C legt die Regularization Strength fest, d.h. wie sehr falsche Vorhersagen \n",
    "# \"bestraft\" werden. Große Werte von C erlauben Fehler und damit komplexere Modelle - dafür ist das Risiko von\n",
    "# \"Overfitting\" erhöht. Kleine Werte von C bestrafen Fehler stärker, d.h. das Modell wird einfacher.\n",
    "# Wenn es zu einfach wird droht \"Underfitting\"\n",
    "\n",
    "log_model = LogisticRegression(C=1.0) # Lege ein Modell an. C ist hier auf dem Default. \n",
    "log_model.fit(X_train, y_train) # trainiere das Modell, sodass es bessere Vorhersagen trifft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zunächst sagen wir unseren Testdatensatz mithilfe des Train Datensatzes vorher. \n",
    "\n",
    "y_pred_train = log_model.predict(X_train)\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "\n",
    "# Nun sagen wir unseren Testdatensatz mithilfe des Test Datensatzes vorher. \n",
    "# Dieser wurde von unserem Modell noch nie gesehen. \n",
    "\n",
    "y_pred_test = log_model.predict(X_test)\n",
    "    \n",
    "log_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(\"\\nTest Accuracy: {}\".format(log_accuracy))\n",
    "\n",
    "# Nun wenden wir die Cross-Validation mit 10 folds an\n",
    "log_cv_accuracy = cross_val_score(log_model, X, y, cv=5, scoring='accuracy')\n",
    "print(\"cross-validation scores\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wenden wir eine Cross-Validation mit 10 folds an. Hier nehmen wir nicht unseren aufgeteilten Datensatz (Xtrain) sondern den Gesamtdatensatz X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nun wenden wir die Cross-Validation mit 10 folds an\n",
    "log_cv_accuracy = cross_val_score(log_model, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "print(\"Cross-validation scores:\", log_cv_accuracy)\n",
    "print(\"\\nCross-validation score mean:\", log_cv_accuracy.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 2. Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1.1 Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legen Sie einen Linear Support Vector Classifier an\n",
    "linear_svc = LinearSVC(C=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainiere das Modell mit unserem Train Datensatz\n",
    "linear_svc = linear_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zuerst sagen wir unsere Zielvariable mithilfe des Test Datensatzes vorher. \n",
    "\n",
    "y_pred_train = linear_svc.predict(X_train)\n",
    "\n",
    "lin_svc_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Train Accuracy: {}\".format(lin_svc_accuracy))\n",
    "\n",
    "# Nun sagen wir unsere Zielvariable mithilfe des Test Datensatzes vorher. \n",
    "\n",
    "y_pred_test = linear_svc.predict(X_test)\n",
    "    \n",
    "svm_lin_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(\"\\nTest Accuracy: {}\".format(svm_lin_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nun wenden wir die Cross-Validation mit 10 folds an\n",
    "linsvc_cv_accuracy = cross_val_score(linear_svc, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "print(\"Cross-validation scores:\", linsvc_cv_accuracy)\n",
    "print(\"\\nCross-validation score mean:\", linsvc_cv_accuracy.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1.2 SVM mit RBF Kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legen Sie einen Support Vector Classifier mit RBF Kernel an\n",
    "rbf_svc = SVC(kernel='rbf', random_state=0, gamma=0.03, C=1.0, probability=True)\n",
    "\n",
    "# Trainieren Sie den Classifier\n",
    "rbf_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zuerst sagen wir unsere Zielvariable mithilfe des Test Datensatzes vorher. \n",
    "\n",
    "y_pred_train = rbf_svc.predict(X_train)\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Train Accuracy: {}\".format(accuracy))\n",
    "\n",
    "# Nun sagen wir unsere Zielvariable mithilfe des Test Datensatzes vorher. \n",
    "\n",
    "y_pred_test = rbf_svc.predict(X_test)\n",
    "    \n",
    "rbf_svc_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(\"\\nTest Accuracy: {}\".format(rbf_svc_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nun wenden wir die Cross-Validation mit 10 folds an\n",
    "rbfsvc_cv_accuracy = cross_val_score(rbf_svc, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "print(\"Cross-validation scores:\", rbfsvc_cv_accuracy)\n",
    "print(\"\\nCross-validation score mean:\", rbfsvc_cv_accuracy.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare all  CV Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methodDict = {}\n",
    "methodDict['Log_CV'] = log_cv_accuracy.mean()\n",
    "methodDict['Linear_SVC_CV'] = linsvc_cv_accuracy.mean()\n",
    "methodDict['RBF_Kernel_CV'] = rbfsvc_cv_accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSuccess():\n",
    "    s = pd.Series(methodDict)\n",
    "    s = s.sort_values(ascending=False)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    #Colors\n",
    "    ax = s.plot(kind='bar') \n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(round(p.get_height(),2)), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
    "    #plt.ylim([30.0, 90.0])\n",
    "    plt.xlabel('Method')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title('Success of methods')\n",
    "     \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set()\n",
    "\n",
    "plotSuccess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weitere Ressourcen:\n",
    "\n",
    "- Chris Albon ist Autor von dem gern genutzten \"Machine Learning with Python Cookbook\". Auf seiner Seite finden sich ziemlich viele und gute Ressourcen https://chrisalbon.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
